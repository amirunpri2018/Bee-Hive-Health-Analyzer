{"cells":[{"metadata":{"_uuid":"9172295963d8f83bd5c2b7204b8511306c4bc791"},"cell_type":"markdown","source":"**The purpose of this kernel is to convert a model from Keras to Tensorflowjs. **\n\n1. This kernel does not use tf.keras. The main model does use it.\n2. The model is converted using the command line conversion tool.\n\nNote that because tf.keras is not being used, the confusion matrix and F1 scores do not match the training and evaluation accuracy. Please refer to the first kernel where this issue is resolved.**"},{"metadata":{"trusted":true,"_uuid":"371731306c3e504b191979706e826c247def88dc","_kg_hide-input":false},"cell_type":"code","source":"from numpy.random import seed\nseed(101)\nfrom tensorflow import set_random_seed\nset_random_seed(101)\n\nimport pandas as pd\nimport numpy as np\nimport keras\n\nfrom keras import backend as K\n#import tensorflow\nfrom keras.layers import Dense, Dropout\nfrom keras.optimizers import Adam\nfrom keras.metrics import categorical_crossentropy\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.models import Model\nfrom keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\nfrom keras.metrics import categorical_accuracy\n\nimport os\nimport cv2\n\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import shuffle\nimport itertools\nimport shutil\nimport matplotlib.pyplot as plt\n%matplotlib inline\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b015ceac61f52106049ab8dee7e10433c1633de8"},"cell_type":"code","source":"# To reduce the class imbalance in the train and validation set, the number of \n# rows in class 'healthy' will be reduced to this number.\n\nHEALTHY_SAMPLE_SIZE = 579\n\n# The approx. total number of images we want in each class after doing image augmentation.\n# We won't be doing image augmentation on the fly.\nNUM_IMAGES_WANTED = 3000 # incl. class 'healthy'\n\n# MobileNet needs input images with shape 224x224x3\nIMAGE_SIZE = 224\nIMAGE_CHANNELS = 3\n\nIMAGE_PATH = \"../input/bee_imgs/bee_imgs/\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dac48f5259e7cb5d31283a662f8ab8e7c1641794"},"cell_type":"code","source":"# What files are available?\n\nos.listdir(\"../input\")","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f2d043abbee34d9d0c02e92231d4bd27e79a58b7"},"cell_type":"markdown","source":"### LABELS\nWhat are the labels and what is the class distribution?"},{"metadata":{"trusted":true,"_uuid":"9f6d1f42790155d5ed901bc3ec4f64c4ee51f87e"},"cell_type":"code","source":"df = pd.read_csv('../input/bee_data.csv')\ndf['health'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"086162161ba405b800863e7d545b5917e5205984"},"cell_type":"markdown","source":"### Create the directory structure\n\nIn these folders we will store the images that will later be fed to the Keras generators. "},{"metadata":{"trusted":true,"_uuid":"d24ef21f9f2359b8bf6b3e7a0b8ab5a43daaf566","_kg_hide-input":false},"cell_type":"code","source":"# Key:\n# healthy = healthy\n# fvar = few varrao, hive beetles\n# var = Varroa, Small Hive Beetles\n# ant = ant problems\n# robbed = hive being robbed\n# queen = missing queen\n\n# Create a new directory\nbase_dir = 'base_dir'\nos.mkdir(base_dir)\n\n\n#[CREATE FOLDERS INSIDE THE BASE DIRECTORY]\n\n# train_dir\n    # 1_healthy\n    # 2_fvar\n    # 3_var\n    # 4_ant\n    # 5_robbed\n    # 6_queen\n \n# val_dir\n    # 1_healthy\n    # 2_fvar\n    # 3_var\n    # 4_ant\n    # 5_robbed\n    # 6_queen\n\n# create a path to 'base_dir' to which we will join the names of the new folders\n\n# train_dir\ntrain_dir = os.path.join(base_dir, 'train_dir')\nos.mkdir(train_dir)\n\n# val_dir\nval_dir = os.path.join(base_dir, 'val_dir')\nos.mkdir(val_dir)\n\n\n# [CREATE FOLDERS INSIDE THE TRAIN AND VALIDATION FOLDERS]\n# Inside each folder we create seperate folders for each class\n\n# create new folders inside train_dir\nhealthy = os.path.join(train_dir, '1_healthy')\nos.mkdir(healthy)\nfvar = os.path.join(train_dir, '2_fvar')\nos.mkdir(fvar)\nvar = os.path.join(train_dir, '3_var')\nos.mkdir(var)\nant = os.path.join(train_dir, '4_ant')\nos.mkdir(ant)\nrobbed = os.path.join(train_dir, '5_robbed')\nos.mkdir(robbed)\nqueen = os.path.join(train_dir, '6_queen')\nos.mkdir(queen)\n\n\n# create new folders inside val_dir\nhealthy = os.path.join(val_dir, '1_healthy')\nos.mkdir(healthy)\nfvar = os.path.join(val_dir, '2_fvar')\nos.mkdir(fvar)\nvar = os.path.join(val_dir, '3_var')\nos.mkdir(var)\nant = os.path.join(val_dir, '4_ant')\nos.mkdir(ant)\nrobbed = os.path.join(val_dir, '5_robbed')\nos.mkdir(robbed)\nqueen = os.path.join(val_dir, '6_queen')\nos.mkdir(queen)\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4ae8d37fdee293aaffa71a79019dd7277f8288fc"},"cell_type":"markdown","source":"### Create Train and Val Sets"},{"metadata":{"trusted":true,"_uuid":"268503398ef61904e05a2c0b0667d589f08a19a8"},"cell_type":"code","source":"df_data = pd.read_csv('../input/bee_data.csv')\n\ndf_data.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3e3a3a5fcabd8f06ed7099c90bfb733496b6de6b"},"cell_type":"markdown","source":"### Downsample the 'healthy' class to reduce the class imbalance"},{"metadata":{"trusted":true,"_uuid":"1a332efcbc2fc282a76094552858b8c79f02da61"},"cell_type":"code","source":"# take a random sample of class 'healthy'\ndf = df_data[df_data['health'] == 'healthy'].sample(HEALTHY_SAMPLE_SIZE, random_state=101)\n\n# remove class 'healthy' from the dataframe\ndf_data = df_data[df_data['health'] != 'healthy']\n\n# concat df and df_data\ndf_data = pd.concat([df_data, df], axis=0).reset_index(drop=True)\n\n# shuffle the new dataframe\ndf_data = shuffle(df_data)\n\n# check the new class distribution\ndf_data['health'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"db267968b6e95ad401a28313fbf3468a53c08f89"},"cell_type":"markdown","source":"### Creat train and val sets"},{"metadata":{"trusted":true,"_uuid":"df735da903622942a61e08391b3c86d6dcdb266f"},"cell_type":"code","source":"\ny = df_data['health']\n\ndf_train, df_val = train_test_split(df_data, test_size=0.1, random_state=101, stratify=y)\n\nprint(df_train.shape)\nprint(df_val.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4b976a9018b1bd2dc0522c68339c5861534a1571"},"cell_type":"code","source":"df_train['health'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1581d5a3e86f9673ae175102112017e30229bc37"},"cell_type":"code","source":"df_val['health'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8812ad87c4fa18d2d82497df42c3895c7f10bc39"},"cell_type":"markdown","source":"### Transfer the Images into the Folders\nWe now transfer the train and val images into the directory structure that we created. Keras needs this directory structure in order to load images from folders into the model during training - and to infer the class of the images."},{"metadata":{"trusted":true,"_uuid":"4acee2b7879762e50b52df118a9b691515fe7ac0"},"cell_type":"code","source":"# Set the 'file' column as the index in df_data\ndf_data.set_index('file', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"eca02fbf066c8124d0cb465295bbd2593f5f045a"},"cell_type":"code","source":"\n# Get a list of train and val images\ntrain_list = list(df_train['file'])\nval_list = list(df_val['file'])\n\n# Key:\n# healthy = healthy\n# fvar = few varrao, hive beetles\n# var = Varroa, Small Hive Beetles\n# ant = ant problems\n# robbed = hive being robbed\n# queen = missing queen\n\n\n\n# Transfer the train images.\n# Note that we re-name the files during the transfer using a number sequence. This is just \n# safety measure help the generators create a repeatable sequence of images \n# and not mix them up thereby giving us confusing results.\n\nlength = len(train_list)\n\nfor i in range(0,length):\n    \n    fname = train_list[i]\n    image = fname\n    description = df_data.loc[image,'health']\n    \n    # map the class descriptions to folder names\n    if description == 'healthy':\n        label = '1_healthy'\n    if description == 'few varrao, hive beetles':\n        label = '2_fvar'\n    if description == 'Varroa, Small Hive Beetles':\n        label = '3_var'\n    if description == 'ant problems':\n        label = '4_ant'\n    if description == 'hive being robbed':\n        label = '5_robbed'\n    if description == 'missing queen':\n        label = '6_queen'\n        \n        \n    # source path to image\n    src = os.path.join('../input/bee_imgs/bee_imgs', fname)\n    # chage the file name\n    new_name = str(i) + '_' + 'train'+ '.png'\n    # destination path to image\n    dst = os.path.join(train_dir, label, new_name)\n    # copy the image from the source to the destination\n    shutil.copyfile(src, dst)\n\n\n# Transfer the val images\n\nlength = len(val_list)\n\nfor i in range(0,length):\n    \n    fname = val_list[i]\n    image = fname\n    description = df_data.loc[image,'health']\n    \n     # map the class descriptions to folder names\n    if description == 'healthy':\n        label = '1_healthy'\n    if description == 'few varrao, hive beetles':\n        label = '2_fvar'\n    if description == 'Varroa, Small Hive Beetles':\n        label = '3_var'\n    if description == 'ant problems':\n        label = '4_ant'\n    if description == 'hive being robbed':\n        label = '5_robbed'\n    if description == 'missing queen':\n        label = '6_queen'\n    \n    # source path to image\n    src = os.path.join('../input/bee_imgs/bee_imgs', fname)\n    # chage the file name\n    new_name = str(i) + '_' + 'val' + '.png'\n    # destination path to image\n    dst = os.path.join(val_dir, label, new_name)\n    # copy the image from the source to the destination\n    shutil.copyfile(src, dst)\n\n        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5a4847c4cc799c23e57bf2531d92117cb95e1b07"},"cell_type":"code","source":"# check how many train images we have in each folder\n\nprint(len(os.listdir('base_dir/train_dir/1_healthy')))\nprint(len(os.listdir('base_dir/train_dir/2_fvar')))\nprint(len(os.listdir('base_dir/train_dir/3_var')))\nprint(len(os.listdir('base_dir/train_dir/4_ant')))\nprint(len(os.listdir('base_dir/train_dir/5_robbed')))\nprint(len(os.listdir('base_dir/train_dir/6_queen')))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fd05c08cbfa00418dc333f5b67d1ff6e98aa973e"},"cell_type":"code","source":"# check how many val images we have in each folder\n\nprint(len(os.listdir('base_dir/val_dir/1_healthy')))\nprint(len(os.listdir('base_dir/val_dir/2_fvar')))\nprint(len(os.listdir('base_dir/val_dir/3_var')))\nprint(len(os.listdir('base_dir/val_dir/4_ant')))\nprint(len(os.listdir('base_dir/val_dir/5_robbed')))\nprint(len(os.listdir('base_dir/val_dir/6_queen')))\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cebcb5242ff542efb03be5086bf3796bea70c591"},"cell_type":"markdown","source":"### Copy the train images  into aug_dir\nWe will not be doing image augmentation on the fly. We will augment the images and then add them to the train folders before training begins. This way the augmented images will be combined with the original images. The training process will also run faster."},{"metadata":{"trusted":true,"_uuid":"8fe970d74e9d5a284420af4ad37d8aae89dc1c15"},"cell_type":"code","source":"# note that we are not augmenting class 'healthy'\nclass_list = ['1_healthy', '2_fvar','3_var','4_ant','5_robbed','6_queen']\n\nfor item in class_list:\n    \n    # We are creating temporary directories here because we delete these directories later.\n    \n    # create a base dir\n    aug_dir = 'aug_dir'\n    os.mkdir(aug_dir)\n    # create a dir within the base dir to store images of the same class\n    img_dir = os.path.join(aug_dir, 'img_dir')\n    os.mkdir(img_dir)\n\n    # Choose a class\n    img_class = item\n\n    # list all images in that directory\n    img_list = os.listdir('base_dir/train_dir/' + img_class)\n\n    # Copy images from the class train dir to the img_dir e.g. class 'healthy'\n    for fname in img_list:\n            # source path to image\n            src = os.path.join('base_dir/train_dir/' + img_class, fname)\n            # destination path to image\n            dst = os.path.join(img_dir, fname)\n            # copy the image from the source to the destination\n            shutil.copyfile(src, dst)\n\n\n    # point to a dir containing the images and NOT to the images themselves\n    path = aug_dir\n    save_path = 'base_dir/train_dir/' + img_class\n\n    # Create a data generator to generate augmented images for each class.\n    datagen = ImageDataGenerator(\n        #rotation_range=180,\n        width_shift_range=0.1,\n        height_shift_range=0.1,\n        zoom_range=0.01,\n        #horizontal_flip=True,\n        #vertical_flip=True,\n        brightness_range=(0.9,1.1),\n        fill_mode='nearest')\n\n    batch_size = 9\n\n    aug_datagen = datagen.flow_from_directory(path,\n                                        save_to_dir=save_path, # this is where the images are saved\n                                        save_format='jpg',\n                                        target_size=(IMAGE_SIZE,IMAGE_SIZE),\n                                        batch_size=batch_size)\n\n\n\n    # Generate the augmented images and add them to the training folders\n    \n    # NUM_IMAGES_WANTED = total number of images we want to have in each class\n    # We will use image augmentation to create the additional images.\n    \n    num_files = len(os.listdir(img_dir))\n    \n    # Just a calculation to get approx. the same amount of images for each class.\n    num_batches = int(np.ceil((NUM_IMAGES_WANTED-num_files)/batch_size))\n\n    # Run the generator and create augmented images.\n    # Note that these images are automatically stored in a folder. The path\n    # to the save folder is specified as a parameter in the generator above.\n    for i in range(0,num_batches):\n\n        imgs, labels = next(aug_datagen)\n        \n    # delete temporary directory with the raw image files\n    shutil.rmtree('aug_dir')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b9bbc56bd25441150d2430dca2b07d8ebae57d95"},"cell_type":"code","source":"# Check how many train images we now have in each folder.\n# This is the original images plus the augmented images.\n\nprint(len(os.listdir('base_dir/train_dir/1_healthy')))\nprint(len(os.listdir('base_dir/train_dir/2_fvar')))\nprint(len(os.listdir('base_dir/train_dir/3_var')))\nprint(len(os.listdir('base_dir/train_dir/4_ant')))\nprint(len(os.listdir('base_dir/train_dir/5_robbed')))\nprint(len(os.listdir('base_dir/train_dir/6_queen')))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"21de03bdc63ecf78cc061d364d14d3216a544b43"},"cell_type":"code","source":"# Check how many val images we have in each folder.\n\nprint(len(os.listdir('base_dir/val_dir/1_healthy')))\nprint(len(os.listdir('base_dir/val_dir/2_fvar')))\nprint(len(os.listdir('base_dir/val_dir/3_var')))\nprint(len(os.listdir('base_dir/val_dir/4_ant')))\nprint(len(os.listdir('base_dir/val_dir/5_robbed')))\nprint(len(os.listdir('base_dir/val_dir/6_queen')))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"767cb7d35e301369f020cdbb705da1620ba8e594"},"cell_type":"markdown","source":"### Visualize a batch of augmented images"},{"metadata":{"trusted":true,"_uuid":"5f0e13a8455af926fe449e1b3ea818b704724202"},"cell_type":"code","source":"# plots images with labels within jupyter notebook\n# source: https://github.com/smileservices/keras_utils/blob/master/utils.py\n\ndef plots(ims, figsize=(12,6), rows=2, interp=False, titles=None): # 12,6\n    if type(ims[0]) is np.ndarray:\n        ims = np.array(ims).astype(np.uint8)\n        if (ims.shape[-1] != 3):\n            ims = ims.transpose((0,2,3,1))\n    f = plt.figure(figsize=figsize)\n    cols = len(ims)//rows if len(ims) % 2 == 0 else len(ims)//rows + 1\n    for i in range(len(ims)):\n        sp = f.add_subplot(rows, cols, i+1)\n        sp.axis('Off')\n        if titles is not None:\n            sp.set_title(titles[i], fontsize=16)\n        plt.imshow(ims[i], interpolation=None if interp else 'none')\n        \nplots(imgs, titles=None) # titles=labels will display the image labels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c3e2126a39c06568a1f95da2ab42353447d1be20"},"cell_type":"code","source":"# End of Data Preparation\n### ===================================================================================== ###\n# Start of Model Building","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"32dad10b7c104d2baa972da8cbadc7d6038af05c"},"cell_type":"markdown","source":"### Set Up the Generators"},{"metadata":{"trusted":true,"_uuid":"aa1041d69b0e8313324b91e3e9475799e1ad61c2"},"cell_type":"code","source":"train_path = 'base_dir/train_dir'\nvalid_path = 'base_dir/val_dir'\n\nnum_train_samples = len(df_train)\nnum_val_samples = len(df_val)\ntrain_batch_size = 10\nval_batch_size = 10\nimage_size = 224\n\ntrain_steps = np.ceil(num_train_samples / train_batch_size)\nval_steps = np.ceil(num_val_samples / val_batch_size)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"edcafa3869045130020d364c640959622459977a"},"cell_type":"markdown","source":"#### A Note on Image Pre-propcessing to Suit MobileNet\n\nWe will be applying the same pre-processing to the bee images as was applied to the original ImageNet  images that were used to train MobileNet. We will add this pre-processing as a preprocessing_function in the generators below."},{"metadata":{"trusted":true,"_uuid":"d0e5aede7139196b0d4e1344b278e7621f005550"},"cell_type":"code","source":"\ndatagen = ImageDataGenerator(preprocessing_function=\n                             keras.applications.mobilenet.preprocess_input)\n\ntrain_gen = datagen.flow_from_directory(train_path,\n                                        target_size=(image_size,image_size),\n                                        batch_size=train_batch_size,\n                                        class_mode='categorical')\n\nval_gen = datagen.flow_from_directory(valid_path,\n                                        target_size=(image_size,image_size),\n                                        batch_size=val_batch_size,\n                                        class_mode='categorical')\n\n# Note: shuffle=False causes the test dataset to not be shuffled.\n# Here we will be using the val set as the test dataset because we need to run predict\n# in order to generate the confusion matrix.\ntest_gen = datagen.flow_from_directory(valid_path,\n                                        target_size=(image_size,image_size),\n                                        batch_size=1,\n                                        class_mode='categorical',\n                                        shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8ee4ee41f1b16083bd9fc20ee9dec40acccc97dd"},"cell_type":"markdown","source":"### Modify MobileNet Model"},{"metadata":{"trusted":true,"_uuid":"ad582cb8ea0ca2d563fc367aa89b7edfafc1a57f"},"cell_type":"code","source":"# Create a copy of a mobilenet model.\n# Please ensure your kaggle kernel is set to 'Internet Connected'.\n\nmobile = keras.applications.mobilenet.MobileNet()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"960449ec7ecdda92ba733ad23b00b7be605f3d4b","_kg_hide-output":true},"cell_type":"code","source":"mobile.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5b7922bdf625675834d9b63ec0e85351bd9f3c0f"},"cell_type":"code","source":"type(mobile.layers)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f832e5865c65a013a06dbf5d500c0381020c56d5"},"cell_type":"code","source":"# How many layers does MobileNet have?\nlen(mobile.layers)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4dd9dcf26d85a57a113e6b158cf8fceeca7f99de"},"cell_type":"code","source":"# CREATE THE MODEL ARCHITECTURE\n\n# Exclude the last 5 layers of the above model.\n# This will include all layers up to and including global_average_pooling2d_1\nx = mobile.layers[-6].output\n\n# Create a new dense layer for predictions\n# 6 corresponds to the number of classes\n#x = Dropout(0.25)(x)\npredictions = Dense(6, activation='softmax')(x)\n\n# inputs=mobile.input selects the input layer, outputs=predictions refers to the\n# dense layer we created above.\n\nmodel = Model(inputs=mobile.input, outputs=predictions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b38734b72afc4289ab187a9e683cbda6bf3269bc","_kg_hide-output":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a9d74e44630c3d07a596460c8fbfda3ae7cae1e9"},"cell_type":"code","source":"# We need to choose how many layers we actually want to be trained.\n\n# Here we are freezing the weights of all layers except the\n# last 23 layers in the new model.\n# The last 23 layers of the model will be trained.\n\nfor layer in model.layers[:-23]:\n    layer.trainable = False","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"13cf63a53e5195cb8a9725d2506c71108bc478b9"},"cell_type":"markdown","source":"### Train the Model"},{"metadata":{"trusted":true,"_uuid":"2013ff1abae70fed845af94e7ab3d95cefad0d61"},"cell_type":"code","source":"model.compile(Adam(lr=0.001), loss='categorical_crossentropy', \n              metrics=['accuracy'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4a5e3bc3cf44f1d4326c34ad880a302ba082e9d5","scrolled":false,"_kg_hide-output":true},"cell_type":"code","source":"\nfilepath = \"model.h5\"\ncheckpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, \n                             save_best_only=True, mode='max')\n\nreduce_lr = ReduceLROnPlateau(monitor='val_acc', factor=0.5, patience=2, \n                                   verbose=1, mode='max', min_lr=0.00001)\n                              \n                              \ncallbacks_list = [checkpoint, reduce_lr]\n\nhistory = model.fit_generator(train_gen, steps_per_epoch=train_steps, \n                            validation_data=val_gen,\n                            validation_steps=val_steps,\n                            epochs=30, verbose=1,\n                           callbacks=callbacks_list)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c3e43e3f2943db4be9d75831fe23661ae9deb44b"},"cell_type":"markdown","source":"### Evaluate the model using the val set\n"},{"metadata":{"trusted":true,"_uuid":"710ee26097924153647ac432c8ade29383fe42f1"},"cell_type":"code","source":"# Get the metric names so that we can see what the output from evaulate_generator will be.\nmodel.metrics_names","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8cb4ce7768cb0eb37f128178e173bface9bb267c"},"cell_type":"markdown","source":"We see that the output metrics are loss and accuracy. Therefore, we now know that model.evaluate_generator() outputs val loss and val accuracy. "},{"metadata":{"trusted":true,"_uuid":"897f066da922d81fefa165a6b911a741c52ef7f5"},"cell_type":"code","source":"# Here the best epoch will be used.\n\nmodel.load_weights('model.h5')\n\nval_loss, val_acc = \\\nmodel.evaluate_generator(test_gen, \n                        steps=len(df_val))\n\nprint('val_loss:', val_loss)\nprint('val_acc:', val_acc)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c3fffba5e0aa9088cda1865c7b8d75d72c20d0f6"},"cell_type":"markdown","source":"### Plot the Training Curves"},{"metadata":{"trusted":true,"_uuid":"0cbd11ef4286a751ef2918361af035d356f341ae"},"cell_type":"code","source":"# display the loss and accuracy curves\n\nimport matplotlib.pyplot as plt\n\nacc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1, len(acc) + 1)\n\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.figure()\n\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\nplt.figure()\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4204e4056c8d12c1fee72b97912879cad4ee483f"},"cell_type":"markdown","source":"### Create a Confusion Matrix\n\nThe confusion matrix and F1 score will tell us how well our model is able to perform on each individual class. "},{"metadata":{"trusted":true,"_uuid":"701dafc5874aa60a054a74c04170cb7e8d750e94"},"cell_type":"code","source":"# make a prediction\n\npredictions = model.predict_generator(test_gen, steps=len(df_val), verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"dcce17ac0488ff90d29b11592c9226ed1bb210fb"},"cell_type":"code","source":"# Get the index of the class with the highest probability score\ny_pred = np.argmax(predictions, axis=1)\n\n# Get the labels of the test images.\ny_true = test_gen.classes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7cfd9bdbbd27e27d9c5de7c6593527686445ea89","_kg_hide-input":true},"cell_type":"code","source":"# Source: Scikit Learn website\n# http://scikit-learn.org/stable/auto_examples/\n# model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-\n# selection-plot-confusion-matrix-py\n\n\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.figure(figsize=(100,100))\n    plt.tight_layout()\n\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"940b71bb2b37d847ba81dd67ca50c7fd5785fd35"},"cell_type":"code","source":"# argmax returns the index of the max value in a row\ncm = confusion_matrix(y_true, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"97c6b493c368ff6565782c1bb15827f5d349ef79"},"cell_type":"code","source":"test_gen.class_indices","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0ddbd33a93468075c64ba49188a6d272a5c7828f"},"cell_type":"code","source":"# Key:\n# healthy = healthy\n# fvar = few varrao, hive beetles\n# var = Varroa, Small Hive Beetles\n# ant = ant problems\n# robbed = hive being robbed\n# queen = missing queen\n\n# Define the labels of the class indices. These need to match the \n# order shown above.\ncm_plot_labels = ['1_healthy', '2_fvar', '3_var', '4_ant', '5_robbed','6_queen']\n\nplot_confusion_matrix(cm, cm_plot_labels, title='Confusion Matrix')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a478ccbc3ba97e21308448b2963b1b9b907e6295"},"cell_type":"code","source":"len(df_val)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0fd36d09f28ec1db282736664b59858411c81bd6"},"cell_type":"markdown","source":"### Generate the Classification Report"},{"metadata":{"trusted":true,"_uuid":"324404b4febf12c31b8f0dd959e177ceae002e66"},"cell_type":"code","source":"# Get the filenames, labels and associated predictions\n\n# This outputs the sequence in which the generator processed the test images\ntest_filenames = test_gen.filenames\n\n# Get the true labels\n#y_true = test_gen.classes\n\n# Get the predicted labels\n#y_pred = predictions.argmax(axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"14a8978010ccb767006e63d6a5ca4e9026a292bb"},"cell_type":"code","source":"from sklearn.metrics import classification_report\n\n# Generate a classification report\n\nnames = ['healthy', 'few varrao, hive beetles', 'Varroa, Small Hive Beetles', \n               'ant problems', 'hive being robbed','missing queen']\n\nreport = classification_report(y_true, y_pred, target_names=names)\n\nprint(report)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b462feaa2de3c325b9d9f484facfdb71e6828a78"},"cell_type":"markdown","source":"**Recall ** = Given a class, will the classifier be able to detect it?<br>\n**Precision** = Given a class prediction from a classifier, how likely is it to be correct?<br>\n**F1 Score** = The harmonic mean of the recall and precision. Essentially, it punishes extreme values.\n"},{"metadata":{"trusted":true,"_uuid":"4c46f3f1d257241f96b4aac7eb96831ff8bbea33"},"cell_type":"code","source":"# End of Model Building\n### ===================================================================================== ###\n# Convert the Model from Keras to Tensorflow.js","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7f9017d69bf0b84522e34841c1876b613cae1535"},"cell_type":"markdown","source":"### Install Tensorflow.js"},{"metadata":{"trusted":true,"_uuid":"2da93a52657b786a8eb7a0d5df6d6a2bcbd0f1c6","_kg_hide-output":true},"cell_type":"code","source":"#!pip install \"tensorflowjs>=0.6.5\"\n!pip install tensorflowjs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d45eff81d2cfec670c485cba15a3cab330f1f627"},"cell_type":"code","source":"# This line gives an error. That's why we have to use the command line conversion tool.\n# This used to work previously in a kaggle kernel. Now it doesn't work.\n\n#import tensorflowjs as tfjs\n\n\n## Error:\n# AttributeError: module 'tensorflow.python.data.ops.dataset_ops' has no attribute 'UnaryDataset'\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a85d7889e2bada2ebe3b84fc1571a89b1a66b7b0"},"cell_type":"markdown","source":"### Convert the model from Keras to Tensorflow.js"},{"metadata":{"trusted":true,"_uuid":"45c3d3f0735be086f883ff4017d219c9235ed076"},"cell_type":"code","source":"# Use the command line conversion tool to convert the model\n\n!tensorflowjs_converter --input_format keras model.h5 tfjs/model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"66ab86338390c5ac053af5f830f77ffea9168846"},"cell_type":"code","source":"os.listdir('tfjs/model')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7df391a2792ddfb7fa2a980776aeac744612f702"},"cell_type":"code","source":"# check the the directory containing the model is available\n!ls","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f774cd15c6de188d4bb150f25ab600e5cbc06031"},"cell_type":"code","source":"# Delete the image data directory we created to prevent a Kaggle error.\n# Kaggle allows a max of 500 files to be saved.\n\nshutil.rmtree('base_dir')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d3fd098aae4fdd509942e58cf61a3bb338f5d4ac"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}